{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eXXoOVTsrYH",
        "outputId": "9e81a879-48dd-4979-cd41-b2e20cd69ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-d8e853f79652>:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n",
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: TFRT_CPU_0\n"
          ]
        }
      ],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## To run JAX on TPU in Google Colab, uncomment the two lines below\n",
        "# import jax.tools.colab_tpu\n",
        "# jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "## JAX\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "# Seeding for random operations\n",
        "main_rng = random.PRNGKey(42)\n",
        "\n",
        "## Flax (NN in JAX)\n",
        "try:\n",
        "    import flax\n",
        "except ModuleNotFoundError: # Install flax if missing\n",
        "    !pip install --quiet flax\n",
        "    import flax\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state, checkpoints\n",
        "\n",
        "## Optax (Optimizers in JAX)\n",
        "try:\n",
        "    import optax\n",
        "except ModuleNotFoundError: # Install optax if missing\n",
        "    !pip install --quiet optax\n",
        "    import optax\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../../saved_models/tutorial7_jax\"\n",
        "\n",
        "print(\"Device:\", jax.devices()[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/JAX/tutorial7/\"\n",
        "# Files to download\n",
        "pretrained_files = []\n",
        "\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please contact the author with the full output including the following error:\\n\", e)"
      ],
      "metadata": {
        "id": "qmrp8bTvErJ3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNLayer(nn.Module):\n",
        "    c_out : int  # Output feature size\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, node_feats, adj_matrix):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Array with node features of shape [batch_size, num_nodes, c_in]\n",
        "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
        "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections.\n",
        "                         Shape: [batch_size, num_nodes, num_nodes]\n",
        "        \"\"\"\n",
        "        # Num neighbours = number of incoming edges\n",
        "        num_neighbours = adj_matrix.sum(axis=-1, keepdims=True)\n",
        "        node_feats = nn.Dense(features=self.c_out, name='projection')(node_feats)\n",
        "        node_feats = jax.lax.batch_matmul(adj_matrix, node_feats)\n",
        "        node_feats = node_feats / num_neighbours\n",
        "        return node_feats"
      ],
      "metadata": {
        "id": "X3Usq9pVWHiC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_feats = jnp.arange(8, dtype=jnp.float32).reshape((1, 4, 2))\n",
        "adj_matrix = jnp.array([[[1, 1, 0, 0],\n",
        "                            [1, 1, 1, 1],\n",
        "                            [0, 1, 1, 1],\n",
        "                            [0, 1, 1, 1]]]).astype(jnp.float32)\n",
        "\n",
        "print(\"Node features:\\n\", node_feats)\n",
        "print(\"\\nAdjacency matrix:\\n\", adj_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRk_AlazWUSN",
        "outputId": "906317fb-80b1-4b41-b938-76a1a4f43414"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node features:\n",
            " [[[0. 1.]\n",
            "  [2. 3.]\n",
            "  [4. 5.]\n",
            "  [6. 7.]]]\n",
            "\n",
            "Adjacency matrix:\n",
            " [[[1. 1. 0. 0.]\n",
            "  [1. 1. 1. 1.]\n",
            "  [0. 1. 1. 1.]\n",
            "  [0. 1. 1. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = GCNLayer(c_out=2)\n",
        "# We define our own parameters here instead of using random initialization\n",
        "params = {'projection': {\n",
        "    'kernel': jnp.array([[1., 0.], [0., 1.]]),\n",
        "    'bias': jnp.array([0., 0.])\n",
        "}}\n",
        "out_feats = layer.apply({'params': params}, node_feats, adj_matrix)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH1wjmrLWZH7",
        "outputId": "a3110058-7635-47dd-867a-4cad87f02ffd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency matrix [[[1. 1. 0. 0.]\n",
            "  [1. 1. 1. 1.]\n",
            "  [0. 1. 1. 1.]\n",
            "  [0. 1. 1. 1.]]]\n",
            "Input features [[[0. 1.]\n",
            "  [2. 3.]\n",
            "  [4. 5.]\n",
            "  [6. 7.]]]\n",
            "Output features [[[1. 2.]\n",
            "  [3. 4.]\n",
            "  [4. 5.]\n",
            "  [4. 5.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention based Graphical Neural Network"
      ],
      "metadata": {
        "id": "vZDOtDRpXu0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GATLayer(nn.Module):\n",
        "    c_out : int  # Dimensionality of output features\n",
        "    num_heads : int  # Number of heads, i.e. attention mechanisms to apply in parallel.\n",
        "    concat_heads : bool = True  # If True, the output of the different heads is concatenated instead of averaged.\n",
        "    alpha : float = 0.2  # Negative slope of the LeakyReLU activation.\n",
        "\n",
        "    def setup(self):\n",
        "        if self.concat_heads:\n",
        "            assert self.c_out % self.num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
        "            c_out_per_head = self.c_out // self.num_heads\n",
        "        else:\n",
        "            c_out_per_head = self.c_out\n",
        "\n",
        "        # Sub-modules and parameters needed in the layer\n",
        "        self.projection = nn.Dense(c_out_per_head * self.num_heads,\n",
        "                                   kernel_init=nn.initializers.glorot_uniform())\n",
        "        self.a = self.param('a',\n",
        "                            nn.initializers.glorot_uniform(),\n",
        "                            (self.num_heads, 2 * c_out_per_head))  # One per head\n",
        "\n",
        "\n",
        "    def __call__(self, node_feats, adj_matrix, print_attn_probs=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
        "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
        "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes = node_feats.shape[0], node_feats.shape[1]\n",
        "\n",
        "        # Apply linear layer and sort nodes by head\n",
        "        node_feats = self.projection(node_feats)\n",
        "        node_feats = node_feats.reshape((batch_size, num_nodes, self.num_heads, -1))\n",
        "\n",
        "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
        "        # In order to take advantage of JAX's just-in-time compilation, we should not use\n",
        "        # arrays with shapes that depend on e.g. the number of edges. Hence, we calculate\n",
        "        # the logit for every possible combination of nodes. For efficiency, we can split\n",
        "        # a[Wh_i||Wh_j] = a_:d/2 * Wh_i + a_d/2: * Wh_j.\n",
        "        logit_parent = (node_feats * self.a[None,None,:,:self.a.shape[0]//2]).sum(axis=-1)\n",
        "        logit_child = (node_feats * self.a[None,None,:,self.a.shape[0]//2:]).sum(axis=-1)\n",
        "        attn_logits = logit_parent[:,:,None,:] + logit_child[:,None,:,:]\n",
        "        attn_logits = nn.leaky_relu(attn_logits, self.alpha)\n",
        "\n",
        "        # Mask out nodes that do not have an edge between them\n",
        "        attn_logits = jnp.where(adj_matrix[...,None] == 1.,\n",
        "                                attn_logits,\n",
        "                                jnp.ones_like(attn_logits) * (-9e15))\n",
        "\n",
        "        # Weighted average of attention\n",
        "        attn_probs = nn.softmax(attn_logits, axis=2)\n",
        "        if print_attn_probs:\n",
        "            print(\"Attention probs\\n\", attn_probs.transpose(0, 3, 1, 2))\n",
        "        node_feats = jnp.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
        "\n",
        "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
        "        if self.concat_heads:\n",
        "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
        "        else:\n",
        "            node_feats = node_feats.mean(axis=2)\n",
        "\n",
        "        return node_feats"
      ],
      "metadata": {
        "id": "HLCVrHKXXh77"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = GATLayer(2, num_heads=2)\n",
        "params = {\n",
        "    'projection': {\n",
        "        'kernel': jnp.array([[1., 0.], [0., 1.]]),\n",
        "        'bias': jnp.array([0., 0.])\n",
        "    },\n",
        "    'a': jnp.array([[-0.2, 0.3], [0.1, -0.1]])\n",
        "}\n",
        "out_feats = layer.apply({'params': params}, node_feats, adj_matrix, print_attn_probs=True)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i1pdcr1XrT_",
        "outputId": "dd8b05a4-b54f-4baa-aba2-97e95facb32d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention probs\n",
            " [[[[0.35434368 0.6456563  0.         0.        ]\n",
            "   [0.10956531 0.14496915 0.264151   0.48131454]\n",
            "   [0.         0.18580715 0.28850412 0.5256887 ]\n",
            "   [0.         0.23912403 0.26961157 0.49126434]]\n",
            "\n",
            "  [[0.5099987  0.49000132 0.         0.        ]\n",
            "   [0.2975179  0.24358703 0.23403586 0.22485918]\n",
            "   [0.         0.38382432 0.31424877 0.3019269 ]\n",
            "   [0.         0.40175956 0.3289329  0.2693075 ]]]]\n",
            "Adjacency matrix [[[1. 1. 0. 0.]\n",
            "  [1. 1. 1. 1.]\n",
            "  [0. 1. 1. 1.]\n",
            "  [0. 1. 1. 1.]]]\n",
            "Input features [[[0. 1.]\n",
            "  [2. 3.]\n",
            "  [4. 5.]\n",
            "  [6. 7.]]]\n",
            "Output features [[[1.2913126 1.9800026]\n",
            "  [4.2344294 3.7724724]\n",
            "  [4.679763  4.836205 ]\n",
            "  [4.50428   4.7350955]]]\n"
          ]
        }
      ]
    }
  ]
}